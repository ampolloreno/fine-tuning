{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2b1c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Title: Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets\n",
      "Length of the Title: 69\n",
      "Length of Abstract: 1144\n",
      "Length of paper: 29536\n",
      "\n",
      "1\n",
      "Title: Transfer Fine-Tuning: A BERT Case Study\n",
      "Length of the Title: 39\n",
      "Length of Abstract: 1266\n",
      "Length of paper: 47174\n",
      "\n",
      "2\n",
      "Title: Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer\n",
      "Length of the Title: 58\n",
      "Length of Abstract: 885\n",
      "Length of paper: 41474\n",
      "\n",
      "3\n",
      "Title: P^3 Ranker: Mitigating the Gaps between Pre-training and Ranking\n",
      "  Fine-tuning with Prompt-based Learning and Pre-finetuning\n",
      "Length of the Title: 124\n",
      "Length of Abstract: 1166\n",
      "Length of paper: 34648\n",
      "\n",
      "4\n",
      "Title: Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A\n",
      "  Prompt-Based Uncertainty Propagation Approach\n",
      "Length of the Title: 116\n",
      "Length of Abstract: 982\n",
      "Length of paper: 83301\n",
      "\n",
      "5\n",
      "Title: Prototypical Fine-tuning: Towards Robust Performance Under Varying Data\n",
      "  Sizes\n",
      "Length of the Title: 79\n",
      "Length of Abstract: 840\n",
      "Length of paper: 43104\n",
      "\n",
      "6\n",
      "Title: Singular Value Fine-tuning: Few-shot Segmentation requires\n",
      "  Few-parameters Fine-tuning\n",
      "Length of the Title: 87\n",
      "Length of Abstract: 1147\n",
      "Length of paper: 69761\n",
      "\n",
      "7\n",
      "Title: The Fine-Tuning of the Universe for Intelligent Life\n",
      "Length of the Title: 52\n",
      "Length of Abstract: 1606\n",
      "Length of paper: 254963\n",
      "\n",
      "8\n",
      "Total so far: 603961\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import feedparser\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "topic = 'fine-tuning'\n",
    "n = 10\n",
    "local_folder = './arxiv_papers'\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "def matches_conference_criteria(comment):\n",
    "    conference_words = ['proceedings', 'conference', 'workshop', 'symposium',\n",
    "                        'journal', 'accepted', 'to appear', 'NeurIPS', 'CVPR',\n",
    "                        'ICML', 'IJCAI', 'ACL', 'ECCV', 'ICCV', 'SIGGRAPH',\n",
    "                        'CHI', 'KDD', 'NIPS', 'EMNLP', 'AAAI', 'ICLR', 'EuroSys']\n",
    "    return any(word in comment.lower() for word in conference_words)\n",
    "\n",
    "def download_and_extract_text_from_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    reader = PdfReader(BytesIO(response.content))\n",
    "    text = ' '.join([page.extract_text() for page in reader.pages])\n",
    "    return text\n",
    "\n",
    "def print_paper_info(entry):\n",
    "    print(f'Title: {entry[\"title\"]}')\n",
    "    print(f'Length of the Title: {len(entry[\"title\"])}')\n",
    "    print(f'Length of Abstract: {len(entry[\"summary\"])}')\n",
    "\n",
    "def pull_papers(n):\n",
    "    response = feedparser.parse(f'http://export.arxiv.org/api/query?search_query=all:{topic}&start=0&max_results=100')\n",
    "    global running_total\n",
    "    count = 0\n",
    "    papers = []\n",
    "    print(len(response.entries))\n",
    "    for entry in response.entries:\n",
    "        if count >= n:\n",
    "            break\n",
    "        paper_comment = entry.get(\"arxiv_comment\")\n",
    "        if paper_comment and matches_conference_criteria(paper_comment):\n",
    "            print_paper_info(entry)\n",
    "            pdf_url = [l['href'] for l in entry.links if l['type'] == 'application/pdf'][0]\n",
    "            text = download_and_extract_text_from_pdf(pdf_url)\n",
    "            length = len(text)\n",
    "            print(f'Length of paper: {length}\\n')\n",
    "            running_total += length\n",
    "            count += 1\n",
    "            paper = {\"title\": entry[\"title\"], \"abstract\": entry[\"summary\"], \"content\": text}\n",
    "            papers.append(paper)\n",
    "            print(count)\n",
    "    return papers\n",
    "            \n",
    "running_total = 0\n",
    "papers = pull_papers(n)\n",
    "print(\"Total so far:\", running_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28eaba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating training set: 100%|██████████████████████| 8/8 [00:00<00:00, 16.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data generated successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import textwrap\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str=\"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Load your paper data here.\n",
    "papers_data = papers\n",
    "\n",
    "# Initialize list to store the training examples and total token count.\n",
    "training_examples = []\n",
    "total_tokens = 0\n",
    "\n",
    "for paper in tqdm(papers_data, desc=\"Creating training set\"):\n",
    "    # Extract paper details\n",
    "    title = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "    content = paper[\"content\"]\n",
    "\n",
    "    # Create the system message\n",
    "    system_msg_text = f\"You are a research assistant. Your job is to carefully read through papers. \\\n",
    "        You have read papers on {topic}, so you have domain knowledge, but you are not an expert, \\\n",
    "        therefore you provide careful arguments with references to specific papers, and explain your reasoning.\"\n",
    "\n",
    "    # Create the user message\n",
    "    user_msg_text = abstract\n",
    "\n",
    "    # Ensure the token count doesn't exceed the limit\n",
    "    system_tokens = num_tokens_from_string(system_msg_text)\n",
    "    user_tokens = num_tokens_from_string(user_msg_text)\n",
    "    token_space = 4000 - system_tokens - user_tokens\n",
    "\n",
    "    # Split the content into chunks that fit into the token limit\n",
    "    content_chunks = textwrap.wrap(content, width=token_space, break_long_words=False)\n",
    "\n",
    "    for chunk in content_chunks:\n",
    "        chunk_tokens = num_tokens_from_string(chunk)\n",
    "        if chunk_tokens > token_space:\n",
    "            print(f\"Skipping paper {title} due to token count exceeding the limit.\")\n",
    "            continue\n",
    "\n",
    "        total_tokens += system_tokens + user_tokens + chunk_tokens\n",
    "        if total_tokens >= 50000000:  # Check the total token limit\n",
    "            print('Reached total token limit for the entire training dataset.')\n",
    "            break\n",
    "\n",
    "        # Create the assistant's message\n",
    "        assistant_msg = {\"role\": \"assistant\", \"content\": f\"{chunk}, and if you want more details you should look in {title}\"}\n",
    "\n",
    "        # Add the conversation to the training examples\n",
    "        example = {\"messages\": [{\"role\": \"system\", \"content\": system_msg_text}, {\"role\": \"user\", \"content\": user_msg_text}, assistant_msg]}\n",
    "        training_examples.append(example)\n",
    "\n",
    "        # Update the user message to continue the conversation in next example\n",
    "        user_msg_text = \"Could you provide more detail?\"\n",
    "\n",
    "        # Update the token space\n",
    "        user_tokens = num_tokens_from_string(user_msg_text)\n",
    "        token_space = 4000 - system_tokens - user_tokens\n",
    "\n",
    "# Write the training examples to a JSONL file\n",
    "with open(\"training_data.jsonl\", \"w\") as f:\n",
    "    for example in training_examples:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print('Training data generated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc85c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "papers = []\n",
    "with open('training_data.jsonl') as f:\n",
    "    for line in f:\n",
    "        papers.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5633799c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 167\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a research assistant. Your job is to carefully read through papers.         You have read papers on fine-tuning, so you have domain knowledge, but you are not an expert,         therefore you provide careful arguments with references to specific papers, and explain your reasoning.'}\n",
      "{'role': 'user', 'content': 'Several datasets have recently been constructed to expose brittleness in\\nmodels trained on existing benchmarks. While model performance on these\\nchallenge datasets is significantly lower compared to the original benchmark,\\nit is unclear what particular weaknesses they reveal. For example, a challenge\\ndataset may be difficult because it targets phenomena that current models\\ncannot capture, or because it simply exploits blind spots in a model\\'s specific\\ntraining set. We introduce inoculation by fine-tuning, a new analysis method\\nfor studying challenge datasets by exposing models (the metaphorical patient)\\nto a small amount of data from the challenge dataset (a metaphorical pathogen)\\nand assessing how well they can adapt. We apply our method to analyze the NLI\\n\"stress tests\" (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and\\nLiang, 2017). We show that after slight exposure, some of these datasets are no\\nlonger challenging, while others remain difficult. Our results indicate that\\nfailures on challenge datasets may lead to very different conclusions about\\nmodels, training datasets, and the challenge datasets themselves.'}\n",
      "{'role': 'assistant', 'content': 'Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets Nelson F. Liu\\x7f~|Roy Schwartz\\x7f|Noah A. Smith\\x7f| \\x7fPaul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA ~Department of Linguistics, University of Washington, Seattle, WA, USA |Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA fnfliu,roysch,nasmith g@cs.washington.edu Abstract Several datasets have recently been con- structed to expose brittleness in models trained on existing benchmarks. While model perfor- mance on these challenge datasets is signiﬁ- cantly lower compared to the original bench- mark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difﬁcult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model’s spe- ciﬁc training set. We introduce inoculation by ﬁne-tuning , a new analysis method for study- ing challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphor- ical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI “stress tests” (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challeng- ing, while others remain difﬁcult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves. 1 Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1manipulations to input data that re- sult in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1Often referred to as “adversarial datasets” or “attacks”. Figure 1: An illustration of the standard challenge eval- uation procedure (e.g., Jia and Liang, 2017) and our proposed analysis method. “Original” refers to the a standard dataset (e.g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. inter alia ). For instance, Naik et al. (2018) gen- erated natural language inference challenge data by applying simple textual transformations to ex- isting examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evalua- tion dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the origi- naldataset ( dataset weakness ). In others, the chal- lenge might expose an inherent inability of a par- ticular model family to handle certain natural lan- guage phenomena ( model weakness ). These are, of course, not mutually exclusive. We introduce inoculation by ﬁne-tuning , aarXiv:1904.02668v4  [cs.CL]  26 Apr 2019 new method for analyzing the effects of challenge datasets (Figure 1).2Given a model trained on the original dataset, we expose it to a small number of examples from the challenge dataset, allowing learning to continue. To the extent that the weak- ness lies with the original dataset, then the inocu- lated model will perform well on both the original and challenge held-out data (Outcome 1 in, and if you want more details you should look in Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "data_path = \"training_data.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602b8b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb60d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2832771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 449, 2550\n",
      "mean / median: 1083.4491017964071, 1028.0\n",
      "p5 / p95: 895.8, 1329.2\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 373, 2474\n",
      "mean / median: 996.5988023952095, 941.0\n",
      "p5 / p95: 819.8, 1228.6\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4228eaed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~180936 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~542808 tokens\n",
      "At 0.008 this is ~43.42464 dollars\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "PRICE_PER_TOKEN = 0.0080\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(f\"At {PRICE_PER_TOKEN} this is ~{PRICE_PER_TOKEN*n_epochs * n_billing_tokens_in_dataset/100} dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5831cc35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.File.create(\n",
    "  file=open(\"training_data.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f69bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "curl_command = f\"\"\"curl https://api.openai.com/v1/files \\\n",
    "  -H \"Authorization: Bearer {apikey}\" \\\n",
    "  -F \"purpose=fine-tune\" \\\n",
    "  -F \"file=@training_data.jsonl\"\"\"\"\n",
    "\n",
    "# Execute the curl command\n",
    "output = subprocess.run(curl_command, shell=True, capture_output=True)\n",
    "\n",
    "\n",
    "# Sample output\n",
    "# {\n",
    "#   \"object\": \"file\",\n",
    "#   \"id\": \"file-swfwefjowkeofewiopefkw\",\n",
    "#   \"purpose\": \"fine-tune\",\n",
    "#   \"filename\": \"training_data.jsonl\",\n",
    "#   \"bytes\": 727141,\n",
    "#   \"created_at\": 1695244917,\n",
    "#   \"status\": \"uploaded\",\n",
    "#   \"status_details\": null\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# The output will be in bytes, so decode it\n",
    "decoded_output = output.stdout.decode('utf-8')\n",
    "\n",
    "# Convert the output to a dictionary using json.loads\n",
    "json_output = json.loads(decoded_output)\n",
    "\n",
    "# Extract the file ID\n",
    "file_id = json_output['id']\n",
    "\n",
    "output = subprocess.run(curl_command, shell=True, capture_output=True)\n",
    "submit_tuning = f\"\"\"curl https://api.openai.com/v1/fine_tuning/jobs \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Authorization: Bearer {apikey}\" \\ \n",
    "-d '{{\n",
    "  \"training_file\": \"{file_id}\",\n",
    "  \"model\": \"gpt-3.5-turbo-0613\"\n",
    "}}'\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
